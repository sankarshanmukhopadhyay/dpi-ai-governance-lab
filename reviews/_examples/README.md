# Example Reviews (Workbench Walkthrough)

This folder contains **example outputs** generated by the DPI–AI Governance Lab workbench to help users understand:

- the **workflow** (extract → scaffold → generate → validate)
- the **deterministic output contract** (the 4 required review artifacts)
- what a **review directory** looks like after a run

> These are **examples for learning and onboarding**, not authoritative assessments.
> They were generated using the **local** engine (offline, deterministic stubs) to keep the repo reproducible without API keys.

## Papers reviewed (examples)

- `cdpi-dpi-ai-framework-2026/` — *DPI–AI Framework: Building AI‑Ready Nations through Digital Public Infrastructure* (CDPI, 2026)
- `tbi-sovereignty-age-of-ai-2026/` — *Sovereignty in the Age of AI: Strategic Choices, Structural Dependencies and the Long Game Ahead* (TBI, Jan 2026)
- `preparing-india-for-ai-adoption/` — *Preparing India for AI Adoption: Challenges and Solutions* (Adarkar & Ganapthy)
- `unesco-ai-maturity-framework-2025/` — *AI Maturity Framework: A self‑positioning guide for public administrations* (UNESCO, 2025)

## What to look at in each review directory

Each review directory includes the workbench’s **required outputs**:

- `paper-analysis.md`
- `paper-review-report.md`
- `paper-review-metadata.yaml`
- `paper-review-scorecard.yaml`

…and reproducibility/audit artifacts:

- `paper.pdf` (the input)
- `extracted/` (canonical text + hashes)
- `run/manifest.json` (engine + parameters + hashes)
- `run/prompts/` and `run/responses/` (where model engines persist prompts/responses)

## How these examples were generated

From the repository root:

```bash
pip install -r requirements.txt

# Example (offline/deterministic)
python -m dpi_lab review --engine local \
  --pdf /path/to/paper.pdf \
  --slug my-paper \
  --out reviews/examples-batch

python -m dpi_lab validate reviews/examples-batch/my-paper
```

## How to generate richer outputs (model-backed)

If you want the workbench to generate **contentful** analysis/report outputs, use a model engine (e.g., OpenAI) and provide the relevant API key.

```bash
export OPENAI_API_KEY="..."
python -m dpi_lab review --engine openai --model gpt-5 \
  --pdf /path/to/paper.pdf \
  --slug my-paper \
  --out reviews/examples-batch

python -m dpi_lab validate reviews/examples-batch/my-paper
```

Note: model-backed runs are still **schema-valid and auditable**, but outputs may vary over time due to provider model updates. The workbench persists prompts, parameters, and hashes in `run/` to support replay and audit.
